# -*- mode: ruby -*-
# vi: set ft=ruby :

ENV['VAGRANT_NO_PARALLEL'] = 'yes'

require "yaml"
settings = YAML.load_file "settings.yaml"

IP_SECTIONS = settings["network"]["control_ip"].match(/^([0-9.]+\.)([^.]+)$/)
# First 3 octets including the trailing dot:
IP_NW = IP_SECTIONS.captures[0]
# Last octet excluding all dots:
IP_START = Integer(IP_SECTIONS.captures[1])

CONTROL = settings["nodes"]["control"]["name"]
WORKER = settings["nodes"]["workers"]["name"]

if settings["nodes"]["workers"]["status"] == true
  NUM_WORKER_NODES = settings["nodes"]["workers"]["count"]
end

Vagrant.configure("2") do |config|
  config.vm.provision "shell", env: { "IP_NW" => IP_NW, "IP_START" => IP_START, "CONTROL" => CONTROL,"WORKER" => WORKER }, inline: <<-SHELL
  #sudo dnf update -y
    echo "$IP_NW$((IP_START)) $CONTROL" >> /etc/hosts
      for i in `seq 1 ${NUM_WORKER_NODES}`; do
        echo "$IP_NW$((IP_START+i)) $WORKER${i}" >> /etc/hosts
      done
    SHELL

    #config.ssh.insert_key = false
    config.vm.synced_folder ".", "/vagrant", create: true
    #config.ssh.private_key_path = "C:/Users/deepu/.ssh/id_ed25519"
    #config.ssh.private_key_path = "C:\Users\deepu\.ssh\id_rsa"
    config.ssh.insert_key = false
    config.vm.box = "my-rocky9"
    #config.vm.box = "generic/centos9s"
    config.vm.box_check_update = true

    
    config.vm.define "master" do |master|
        
        master.vm.hostname = "#{CONTROL}"
        master.vm.network "private_network", ip: settings["network"]["control_ip"]
            if settings["shared_folders"]
            settings["shared_folders"].each do |shared_folder|
                master.vm.synced_folder shared_folder["host_path"], shared_folder["vm_path"]
            end
        end
        
        #master.vm.network "private_network", ip: 1
        #centos.vm.network "public_network", use_dhcp_assigned_default_route: true, bridge: "Wi-Fi"

        master.vm.provider "virtualbox" do |vb|
            vb.name = master.vm.hostname
            vb.cpus = settings["nodes"]["control"]["cpu"]
            vb.memory = settings["nodes"]["control"]["memory"]
            
            if settings["cluster_name"] and settings["cluster_name"] != ""
              vb.customize ["modifyvm", :id, "--groups", ("/" + settings["cluster_name"])]
              vb.customize ["modifyvm", :id, "--natdnshostresolver1", "on"]
              vb.customize ["modifyvm", :id, "--ioapic", "on"]
            end
        end
        
        master.vm.provision "shell",
        env: {
            "DNS_SERVERS" => settings["network"]["dns_servers"].join(" ")
        },
        path: "scripts/common.sh"

        if settings["software"]["ec2user"] == true
          master.vm.provision "shell", path: "scripts/ec2-user.sh"
        end

        if settings["software"]["kubernetes"] == true
          master.vm.provision "shell",
              env: {
              "KUBERNETES_VERSION" => settings["software"]["kubernetes"],
              "ENVIRONMENT" => settings["environment"],
              "OS" => settings["software"]["os"]
              },
              path: "scripts/kcommon.sh"
     

          master.vm.provision "shell",
              env: {
                "CALICO_VERSION" => settings["software"]["calico"],
                "CONTROL_IP" => settings["network"]["control_ip"],
                "POD_CIDR" => settings["network"]["pod_cidr"],
                "SERVICE_CIDR" => settings["network"]["service_cidr"]
              },
              path: "scripts/kmaster.sh"
        end
    
        if settings["software"]["jenkins"] == true
          master.vm.provision "shell", path: "scripts/jenkins.sh"
        end

        if settings["software"]["ansible"] == true
          master.vm.provision "shell", path: "scripts/ansible.sh"
        end

        if settings["software"]["docker"] == true
          master.vm.provision "shell", path: "scripts/docker.sh"
        end
        
    end


    if settings["nodes"]["workers"]["status"] == true
      (1..NUM_WORKER_NODES).each do |i|

        config.vm.define "node0#{i}" do |node|
          node.ssh.insert_key = false
          node.vm.hostname = "#{WORKER}#{i}"
          #node.vm.hostname = "worker0#{i}"
          node.vm.network "private_network", ip: IP_NW + "#{IP_START + i}"
          if settings["shared_folders"]
            settings["shared_folders"].each do |shared_folder|
              node.vm.synced_folder shared_folder["host_path"], shared_folder["vm_path"]
            end
          end
          node.vm.provider "virtualbox" do |vb|
              vb.name = node.vm.hostname
              vb.cpus = settings["nodes"]["workers"]["cpu"]
              vb.memory = settings["nodes"]["workers"]["memory"]
              if settings["cluster_name"] and settings["cluster_name"] != ""
                vb.customize ["modifyvm", :id, "--groups", ("/" + settings["cluster_name"])]
              end
          end
            
            node.vm.provision "shell",
              env: {
                "DNS_SERVERS" => settings["network"]["dns_servers"].join(" ")
              },
            path: "scripts/common.sh"
          
            if settings["software"]["ec2user"] == true
              node.vm.provision "shell", path: "scripts/ec2-user.sh"
            end

            if settings["software"]["kubernetes"] == true
              node.vm.provision "shell",
              env: {
                "ENVIRONMENT" => settings["environment"],
                "KUBERNETES_VERSION" => settings["software"]["kubernetes"],
                "OS" => settings["software"]["os"]
              },
              path: "scripts/kcommon.sh"
              
              node.vm.provision "shell", path: "scripts/knode.sh"

            end                          
    
          # Only install the dashboard after provisioning the last worker (and when enabled).
          if i == NUM_WORKER_NODES and settings["software"]["dashboard"] and settings["software"]["dashboard"] != ""
            node.vm.provision "shell", path: "scripts/dashboard.sh"
          end

        end
      
      end

    end

end